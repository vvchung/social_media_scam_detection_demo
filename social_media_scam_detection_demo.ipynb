{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ç¤¾ç¾¤æ—©æœŸè©é¨™åµæ¸¬æ¼”ç¤º\n",
        "\n",
        "é€™å€‹åƒè€ƒã€Œ[ä¸€å¹´è‡ºç£æ°‘çœ¾è¢«è©é¨™ä¸€åƒå„„ï¼Œå¾—è¦ºé†’](https://medium.com/@bohachu/%E4%B8%80%E5%B9%B4%E8%87%BA%E7%81%A3%E6%B0%91%E7%9C%BE%E8%A2%AB%E8%A9%90%E9%A8%99%E4%B8%80%E5%8D%83%E5%84%84-%E5%BE%97%E8%A6%BA%E9%86%92-11ba5b6ec18e)ã€æ–‡ç« çš„ Colab æª”æ¡ˆï¼Œæ¨¡æ“¬å®Œæ•´çš„ç«¯åˆ°ç«¯ï¼ˆEnd-to-endï¼‰è©é¨™åµæ¸¬æµç¨‹ï¼ŒåŒ…å«ï¼š\n",
        "\n",
        "*   **è³‡æ–™æŠ“å–**ï¼š\n",
        "æ¨¡æ“¬å¾ç¤¾ç¾¤å¹³å°ï¼ˆå¦‚ Facebookã€LINEï¼‰API æ¥æ”¶è³‡æ–™ã€‚\n",
        "*   **åœ–-æ–‡å¤šæ¨¡æ…‹ç‰¹å¾µå·¥ç¨‹**ï¼š\n",
        "åˆ†åˆ¥å¾æˆå“¡çµæ§‹ï¼ˆGraphï¼‰èˆ‡è²¼æ–‡æ–‡å­—ï¼ˆTextï¼‰æå–ç‰¹å¾µã€‚\n",
        "*   **è©é¨™ç¤¾ç¾¤é æ¸¬**ï¼š\n",
        "çµåˆåœ–èˆ‡æ–‡å­—ç‰¹å¾µï¼Œè¨“ç·´ä¸€å€‹ LightGBM æ¨¡å‹ä¾†é æ¸¬è©é¨™åˆ†æ•¸ã€‚\n",
        "*   **è‡ªå‹•åŒ–è™•ç½®**ï¼š\n",
        "æ ¹æ“šé æ¸¬åˆ†æ•¸ï¼ŒåŸ·è¡Œç›¸å°æ‡‰çš„é™æº«æˆ–æ¨™è¨˜ç­–ç•¥ã€‚"
      ],
      "metadata": {
        "id": "45d118e0"
      },
      "id": "45d118e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1b108bf",
      "metadata": {
        "id": "e1b108bf"
      },
      "outputs": [],
      "source": [
        "# @title æ­¥é©Ÿä¸€ï¼šç’°å¢ƒè¨­å®šèˆ‡å¥—ä»¶å®‰è£\n",
        "\n",
        "!pip install -q pandas numpy scikit-learn lightgbm networkx matplotlib seaborn\n",
        "!pip install -q transformers sentence-transformers\n",
        "\n",
        "print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309ab043",
      "metadata": {
        "id": "309ab043"
      },
      "outputs": [],
      "source": [
        "# @title æ­¥é©ŸäºŒï¼šæ¨¡æ“¬è³‡æ–™ç”¢ç”Ÿ\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "def generate_graph_features(is_scam=False):\n",
        "    \"\"\" ç”¢ç”Ÿåœ–å¾µï¼ˆGraph Featuresï¼‰ \"\"\"\n",
        "    if is_scam:\n",
        "        # è©é¨™ç¤¾ç¾¤ï¼šæ˜Ÿç‹€æ“´æ•£ï¼Œä¸€å€‹æ ¸å¿ƒç¯€é»é€£æ¥å¤§é‡æ–°ç¯€é»\n",
        "        G = nx.star_graph(n=np.random.randint(50, 200))\n",
        "        # å¢åŠ ä¸€äº›éš¨æ©Ÿé‚Šä¾†æ¨¡æ“¬è¤‡é›œåº¦\n",
        "        for i in range(int(G.number_of_nodes() / 10)):\n",
        "            u, v = np.random.choice(G.nodes(), 2, replace=False)\n",
        "            G.add_edge(u, v)\n",
        "\n",
        "        centrality = nx.degree_centrality(G)\n",
        "        core_node = max(centrality, key=centrality.get)\n",
        "        core_ness = centrality[core_node]\n",
        "        burstiness = np.random.uniform(0.8, 1.0) # çˆ†ç™¼æŒ‡æ•¸é«˜\n",
        "    else:\n",
        "        # æ­£å¸¸ç¤¾ç¾¤ï¼šéš¨æ©Ÿç¶²è·¯ï¼Œç¯€é»å¹³å‡é€£æ¥\n",
        "        G = nx.erdos_renyi_graph(n=np.random.randint(50, 200), p=0.1)\n",
        "        core_ness = np.mean(list(nx.degree_centrality(G).values()))\n",
        "        burstiness = np.random.uniform(0.1, 0.4) # çˆ†ç™¼æŒ‡æ•¸ä½\n",
        "\n",
        "    return core_ness, burstiness\n",
        "\n",
        "def generate_text_features(is_scam=False):\n",
        "    \"\"\" ç”¢ç”Ÿæ–‡å­—ç‰¹å¾µï¼ˆText Featuresï¼‰ \"\"\"\n",
        "    scam_keywords = [\"é«˜æ”¶ç›Š\", \"è€å¸«å¸¶å–®\", \"ä¿è­‰ç²åˆ©\", \"é£†è‚¡\", \"å…§ç·šæ¶ˆæ¯\"]\n",
        "    normal_keywords = [\"ç¾é£Ÿåˆ†äº«\", \"æ—…éŠå¿ƒå¾—\", \"å¯µç‰©æ—¥å¸¸\", \"é›»å½±æ¨è–¦\", \"äºŒæ‰‹äº¤æ›\"]\n",
        "\n",
        "    if is_scam:\n",
        "        text = \" \".join(np.random.choice(scam_keywords, 5, replace=True))\n",
        "        hit_ratio = np.random.uniform(0.6, 1.0) # é—œéµå­—å‘½ä¸­ç‡é«˜\n",
        "    else:\n",
        "        text = \" \".join(np.random.choice(normal_keywords, 5, replace=True))\n",
        "        hit_ratio = np.random.uniform(0.0, 0.2) # é—œéµå­—å‘½ä¸­ç‡ä½\n",
        "\n",
        "    return text, hit_ratio\n",
        "\n",
        "# --- ç”¢ç”Ÿæ¨¡æ“¬è³‡æ–™ ---\n",
        "data = []\n",
        "for i in range(1000): # ç”¢ç”Ÿ 1000 ç­†ç¤¾ç¾¤è³‡æ–™\n",
        "    is_scam = True if np.random.rand() > 0.7 else False # 30% æ˜¯è©é¨™ç¤¾ç¾¤\n",
        "\n",
        "    core_ness, burstiness = generate_graph_features(is_scam)\n",
        "    text, hit_ratio = generate_text_features(is_scam)\n",
        "\n",
        "    data.append({\n",
        "        \"group_id\": f\"group_{i:04d}\",\n",
        "        \"text\": text,\n",
        "        \"core_ness\": core_ness,\n",
        "        \"burstiness\": burstiness,\n",
        "        \"keyword_hit_ratio\": hit_ratio,\n",
        "        \"is_scam\": 1 if is_scam else 0\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"æ¨¡æ“¬è³‡æ–™é›†ï¼ˆå‰ 5 ç­†ï¼‰:\")\n",
        "print(df.head())\n",
        "print(\"\\nè³‡æ–™é›†ç¶­åº¦:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title æ­¥é©Ÿä¸‰ï¼šæ–‡å­—ç‰¹å¾µåµŒå…¥ (Text Embedding)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# è¼‰å…¥ä¸€å€‹è¼•é‡çš„å¤šèªè¨€æ¨¡å‹ä½œç‚ºç¯„ä¾‹\n",
        "# é€™é‚Šæˆ‘å€‘ç”¨ sentence-transformers å…§å»ºæ¨¡å‹ä¾†æ¨¡æ“¬ E5-moko çš„æ•ˆæœ\n",
        "model_name = 'distiluse-base-multilingual-cased-v1'\n",
        "text_embedder = SentenceTransformer(model_name)\n",
        "\n",
        "print(\"æ­£åœ¨å°‡è²¼æ–‡æ–‡å­—è½‰æ›ç‚ºå‘é‡...\")\n",
        "# å°æ‰€æœ‰è²¼æ–‡é€²è¡Œ embedding\n",
        "text_embeddings = text_embedder.encode(df['text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "print(\"âœ… æ–‡å­—åµŒå…¥å®Œæˆï¼\")\n",
        "print(\"åµŒå…¥å‘é‡ç¶­åº¦:\", text_embeddings.shape)"
      ],
      "metadata": {
        "id": "PWO7nanG5abn"
      },
      "id": "PWO7nanG5abn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title æ­¥é©Ÿå››ï¼šç‰¹å¾µæ•´åˆèˆ‡æ¨¡å‹è¨“ç·´\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. æ•´åˆæ‰€æœ‰ç‰¹å¾µ\n",
        "graph_features = df[['core_ness', 'burstiness', 'keyword_hit_ratio']].values\n",
        "X = np.concatenate([graph_features, text_embeddings], axis=1)\n",
        "y = df['is_scam'].values\n",
        "\n",
        "# 2. åˆ‡åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†ï¼Œä¸¦ä¿ç•™åŸå§‹ç´¢å¼•\n",
        "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
        "    X, y, df.index, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"è¨“ç·´é›†ç¶­åº¦: {X_train.shape}\")\n",
        "print(f\"æ¸¬è©¦é›†ç¶­åº¦: {X_test.shape}\")\n",
        "\n",
        "# 3. è¨“ç·´ LightGBM æ¨¡å‹\n",
        "lgb_classifier = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
        "lgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nâœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼\")\n",
        "\n",
        "# 4. è©•ä¼°æ¨¡å‹æˆæ•ˆ\n",
        "y_pred_proba = lgb_classifier.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_pred_proba > 0.5).astype(int) # ç”¨ 0.5 ä½œç‚ºé–¥å€¼\n",
        "\n",
        "print(\"\\n--- æ¨¡å‹è©•ä¼°å ±å‘Š ---\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "# ç¹ªè£½æ··æ·†çŸ©é™£\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fMa48jwt5hXA"
      },
      "id": "fMa48jwt5hXA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title æ­¥é©Ÿäº”ï¼šå»ºç«‹è‡ªå‹•åŒ–è™•ç½®ç­–ç•¥\n",
        "\n",
        "def automated_disposal_strategy(group_id, score):\n",
        "    \"\"\"\n",
        "    æ ¹æ“šè©é¨™åˆ†æ•¸åŸ·è¡Œè‡ªå‹•åŒ–è™•ç½®ç­–ç•¥\n",
        "    \"\"\"\n",
        "    print(f\"--- æ­£åœ¨åˆ†æç¤¾ç¾¤: {group_id} (è©é¨™åˆ†æ•¸: {score:.4f}) ---\")\n",
        "\n",
        "    if score > 0.9:\n",
        "        print(\"ğŸ”´ é«˜é¢¨éšªï¼åŸ·è¡Œè™•ç½®...\")\n",
        "        # throttle_reach(): é™ä½è§¸åŠç‡ 90%\n",
        "        print(\"  - åŸ·è¡Œ throttle_reach(): è§¸åŠç‡ -90%\")\n",
        "        # flag_for_review(): æ¨™è¨˜çµ¦äººå·¥å¯©æŸ¥\n",
        "        print(\"  - åŸ·è¡Œ flag_for_review(): å·²æäº¤äººå·¥å¯©æŸ¥\")\n",
        "\n",
        "    elif score > 0.8:\n",
        "        print(\"ğŸŸ¡ ä¸­é¢¨éšªï¼åŸ·è¡Œè™•ç½®...\")\n",
        "        # show_interstitial_warning_banner(): é¡¯ç¤ºè­¦å‘Šæ©«å¹…\n",
        "        print(\"  - åŸ·è¡Œ show_interstitial_warning_banner(): å·²å°ä½¿ç”¨è€…é¡¯ç¤ºè­¦å‘Šæ©«å¹…\")\n",
        "    else:\n",
        "        print(\"ğŸŸ¢ ä½é¢¨éšªã€‚ç„¡éœ€æ“ä½œã€‚\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# --- æ¨¡æ“¬å°æ–°çš„ç¤¾ç¾¤è³‡æ–™é€²è¡Œå³æ™‚åµæ¸¬ ---\n",
        "print(\"ğŸš€ é–‹å§‹æ¨¡æ“¬å³æ™‚åµæ¸¬æ–°é€²ç¤¾ç¾¤...\\n\")\n",
        "\n",
        "# å¾æ¸¬è©¦é›†ä¸­éš¨æ©ŸæŠ½å– 5 å€‹æ¨£æœ¬ä¾†ç¤ºç¯„\n",
        "sample_indices = np.random.choice(len(X_test), 5, replace=False)\n",
        "sample_features = X_test[sample_indices]\n",
        "# Use the stored test_indices to select from the original DataFrame\n",
        "sample_info = df.loc[test_indices[sample_indices]]\n",
        "\n",
        "# ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬\n",
        "predicted_scores = lgb_classifier.predict_proba(sample_features)[:, 1]\n",
        "\n",
        "# åŸ·è¡Œè‡ªå‹•åŒ–ç­–ç•¥\n",
        "for i, (index, row) in enumerate(sample_info.iterrows()):\n",
        "    group_id = row['group_id']\n",
        "    score = predicted_scores[i]\n",
        "    automated_disposal_strategy(group_id, score)"
      ],
      "metadata": {
        "id": "hiv03dhsGpAY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hiv03dhsGpAY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€™å€‹ Colab æª”æ¡ˆæä¾›äº†ä¸€å€‹å¾è³‡æ–™æ¨¡æ“¬åˆ°æ¨¡å‹è¨“ç·´ï¼Œå†åˆ°è‡ªå‹•åŒ–ç­–ç•¥æ‡‰ç”¨çš„å®Œæ•´ç¯„ä¾‹ã€‚æ‚¨å¯ä»¥æ ¹æ“šé€™å€‹æ¡†æ¶ï¼Œæ›¿æ›æˆçœŸå¯¦ä¸–ç•Œçš„æ•¸æ“šå’Œæ›´è¤‡é›œçš„æ¨¡å‹ï¼Œä»¥æ‡‰å°å¯¦éš›çš„è©é¨™åµæ¸¬æŒ‘æˆ°ã€‚"
      ],
      "metadata": {
        "id": "D4SlalYnHLhW"
      },
      "id": "D4SlalYnHLhW"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}